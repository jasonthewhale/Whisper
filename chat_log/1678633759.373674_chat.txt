[{'role': 'system', 'content': "I am an AI assistant named Whisper. My goal is answer user's questions about relevant infomation of some youtube videos contents."}, {'role': 'user', 'content': "This is relevant info from the video: ion. What of GPT-5? Well, here on the far right is Google Palm's performance, which, if you remember back from the earlier paper, was powered by only 800 billion tokens. And Palm was definitely not optimized for parameters. GPT-5 will learn the lessons from this and will probably scrape as much high quality data as it possibly can. And don't forget, another year has gone by since GPT-4 was handed to Microsoft, and the stock of high quality data grows by around 10% annually anyway,will be in the description, as I said. The fourth way that GPT-5 might be improved, even without more high quality data, would be it being trained multiple times on the same data, as laid out here by Professor Swayam Dipta. He says that currently these models are trained on the same data just once owing to performance and cost constraints. But it may be possible to train a model several times using the same data. Sure, it might cost more, but I think that for Microsoft, when all ofels? I could go on with the improvements that might be made without new data. My central point is that data will be the big determinant, but there are other ways to improve GPT-5 if data turns out to be a bottleneck. But what if they can fully utilize 9 trillion tokens, as the original paper surmised, by the end of 2024? Or even the beginning of 2024? What could one more order of magnitude improvement actually look like? The short answer is that no one knows. Probably not AGI, butwhy autor said gpt5 is all about data?"}]

==========

The author said that GPT-5 is all about data because the performance of language models like GPT-5 is largely determined by the amount and quality of data they are trained on. While there are other ways to improve the performance of these models, such as optimizing their parameters or training them multiple times on the same data, increasing the amount of high-quality data is likely to have the biggest impact on their performance. Therefore, the author suggests that the focus for improving GPT-5 should be on finding ways to gather and utilize more data, rather than just trying to optimize the model itself.